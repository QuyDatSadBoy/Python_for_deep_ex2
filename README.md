# Python for Deep Learning - Exercise 2

ÄÃ¢y lÃ  bÃ i táº­p thá»±c hÃ nh sá»‘ 2 vá» Deep Learning sá»­ dá»¥ng PyTorch, táº­p trung vÃ o viá»‡c xÃ¢y dá»±ng vÃ  triá»ƒn khai má»™t máº¡ng neural network Ä‘a lá»›p (Multi-layer Neural Network).

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

BÃ i táº­p nÃ y yÃªu cáº§u sinh viÃªn triá»ƒn khai:
1. **HÃ m kÃ­ch hoáº¡t (Activation Functions)**: Sigmoid, Tanh, ReLU, hoáº·c Leaky ReLU
2. **HÃ m Softmax**: Äá»ƒ chuyá»ƒn Ä‘á»•i output thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t
3. **Forward Pass**: TÃ­nh toÃ¡n lan truyá»n thuáº­n qua toÃ n bá»™ máº¡ng neural

## ğŸ—ï¸ Kiáº¿n trÃºc máº¡ng Neural Network

Máº¡ng neural Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i kiáº¿n trÃºc sau:

```
Input Layer: 784 nodes (28x28 pixels - tÆ°Æ¡ng á»©ng vá»›i áº£nh MNIST)
    â†“
Hidden Layer 1: 128 nodes
    â†“
Hidden Layer 2: 256 nodes  
    â†“
Hidden Layer 3: 128 nodes
    â†“
Output Layer: 10 nodes (10 classes phÃ¢n loáº¡i)
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
Python_for_deep_ex2/
â”œâ”€â”€ deeplearning_ex2.py          # File bÃ i táº­p (cáº§n hoÃ n thÃ nh)
â”œâ”€â”€ deeplearning_ex2_solution.py # File Ä‘Ã¡p Ã¡n tham kháº£o
â””â”€â”€ README.md                    # File nÃ y
```

## ğŸ”§ YÃªu cáº§u há»‡ thá»‘ng

- Python 3.7+
- PyTorch 1.0+

### CÃ i Ä‘áº·t dependencies

```bash
pip install torch
```

## ğŸ“ Chi tiáº¿t bÃ i táº­p

### 1. Triá»ƒn khai hÃ m kÃ­ch hoáº¡t (activation_func)

Cáº§n triá»ƒn khai má»™t trong cÃ¡c hÃ m kÃ­ch hoáº¡t sau:

- **Sigmoid**: `Ïƒ(x) = 1/(1 + e^(-x))`
- **Tanh**: `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))`
- **ReLU**: `ReLU(x) = max(0, x)`
- **Leaky ReLU**: `LeakyReLU(x) = max(Îµx, x)` vá»›i Îµ = 0.01

```python
def activation_func(x):
    #TODO Implement one of these following activation function: sigmoid, tanh, ReLU, leaky ReLU
    epsilon = 0.01   # Only use this variable if you choose Leaky ReLU
    result = None
    return result
```

### 2. Triá»ƒn khai hÃ m Softmax

HÃ m Softmax chuyá»ƒn Ä‘á»•i vector Ä‘áº§u ra thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t:

```python
def softmax(x):
    # TODO Implement softmax function here
    result = None
    return result
```

**CÃ´ng thá»©c Softmax**: `softmax(x_i) = e^(x_i) / Î£(e^(x_j))`

### 3. TÃ­nh toÃ¡n Forward Pass

Cáº§n triá»ƒn khai quÃ¡ trÃ¬nh lan truyá»n thuáº­n qua toÃ n bá»™ máº¡ng:

```python
#TODO Calculate forward pass of the network here. Result should have the shape of [1,10]
# Dont forget to check if sum of result = 1.0
result = None
```

**Quy trÃ¬nh Forward Pass**:
1. `h1 = activation_func(input_data Ã— W1 + B1)`
2. `h2 = activation_func(h1 Ã— W2 + B2)`
3. `h3 = activation_func(h2 Ã— W3 + B3)`
4. `output = softmax(h3 Ã— W4 + B4)`

## ğŸ¯ Má»¥c tiÃªu há»c táº­p

Sau khi hoÃ n thÃ nh bÃ i táº­p, sinh viÃªn sáº½:

- âœ… Hiá»ƒu cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a cÃ¡c hÃ m kÃ­ch hoáº¡t khÃ¡c nhau
- âœ… Náº¯m vá»¯ng cÃ¡ch triá»ƒn khai hÃ m Softmax
- âœ… Biáº¿t cÃ¡ch tÃ­nh toÃ¡n Forward Pass trong máº¡ng neural Ä‘a lá»›p
- âœ… LÃ m quen vá»›i thÆ° viá»‡n PyTorch cho Deep Learning
- âœ… Hiá»ƒu kiáº¿n trÃºc cá»§a máº¡ng neural network cÆ¡ báº£n

## ğŸš€ CÃ¡ch cháº¡y code

### Cháº¡y file bÃ i táº­p (sau khi hoÃ n thÃ nh):
```bash
python deeplearning_ex2.py
```

### Cháº¡y file Ä‘Ã¡p Ã¡n Ä‘á»ƒ tham kháº£o:
```bash
python deeplearning_ex2_solution.py
```

## âœ… Kiá»ƒm tra káº¿t quáº£

Káº¿t quáº£ Ä‘Ãºng pháº£i thá»a mÃ£n:
- Shape cá»§a output: `[1, 10]`
- Tá»•ng cÃ¡c pháº§n tá»­ trong output = 1.0 (do sá»­ dá»¥ng Softmax)
- Táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ Ä‘á»u náº±m trong khoáº£ng [0, 1]

## ğŸ’¡ Gá»£i Ã½ triá»ƒn khai

### HÃ m Sigmoid (Ä‘á» xuáº¥t):
```python
def activation_func(x):
    return 1 / (1 + torch.exp(-x))
```

### HÃ m Softmax:
```python
def softmax(x):
    max_value, _ = torch.max(x, dim=1)
    norm = torch.exp(x - max_value)
    return norm / norm.sum()
```

## ğŸ“š TÃ i liá»‡u
